{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/ubaydulloasatullaev/random-forests?scriptVersionId=135764182\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"ca72d4e7","metadata":{"papermill":{"duration":0.004335,"end_time":"2023-07-05T04:08:01.280887","exception":false,"start_time":"2023-07-05T04:08:01.276552","status":"completed"},"tags":[]},"source":["**This notebook is an exercise in the [Introduction to Machine Learning](https://www.kaggle.com/learn/intro-to-machine-learning) course.  You can reference the tutorial at [this link](https://www.kaggle.com/dansbecker/random-forests).**\n","\n","---\n"]},{"cell_type":"markdown","id":"72dc15d4","metadata":{"papermill":{"duration":0.003413,"end_time":"2023-07-05T04:08:01.288172","exception":false,"start_time":"2023-07-05T04:08:01.284759","status":"completed"},"tags":[]},"source":["## Recap\n","Here's the code you've written so far."]},{"cell_type":"code","execution_count":1,"id":"e1528743","metadata":{"execution":{"iopub.execute_input":"2023-07-05T04:08:01.297438Z","iopub.status.busy":"2023-07-05T04:08:01.296904Z","iopub.status.idle":"2023-07-05T04:08:02.919946Z","shell.execute_reply":"2023-07-05T04:08:02.918918Z"},"papermill":{"duration":1.631545,"end_time":"2023-07-05T04:08:02.92329","exception":false,"start_time":"2023-07-05T04:08:01.291745","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Validation MAE when not specifying max_leaf_nodes: 29,653\n","Validation MAE for best value of max_leaf_nodes: 27,283\n","\n","Setup complete\n"]}],"source":["# Code you have previously used to load data\n","import pandas as pd\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeRegressor\n","\n","\n","# Path of the file to read\n","iowa_file_path = '../input/home-data-for-ml-course/train.csv'\n","\n","home_data = pd.read_csv(iowa_file_path)\n","# Create target object and call it y\n","y = home_data.SalePrice\n","# Create X\n","features = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\n","X = home_data[features]\n","\n","# Split into validation and training data\n","train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n","\n","# Specify Model\n","iowa_model = DecisionTreeRegressor(random_state=1)\n","# Fit Model\n","iowa_model.fit(train_X, train_y)\n","\n","# Make validation predictions and calculate mean absolute error\n","val_predictions = iowa_model.predict(val_X)\n","val_mae = mean_absolute_error(val_predictions, val_y)\n","print(\"Validation MAE when not specifying max_leaf_nodes: {:,.0f}\".format(val_mae))\n","\n","# Using best value for max_leaf_nodes\n","iowa_model = DecisionTreeRegressor(max_leaf_nodes=100, random_state=1)\n","iowa_model.fit(train_X, train_y)\n","val_predictions = iowa_model.predict(val_X)\n","val_mae = mean_absolute_error(val_predictions, val_y)\n","print(\"Validation MAE for best value of max_leaf_nodes: {:,.0f}\".format(val_mae))\n","\n","\n","# Set up code checking\n","from learntools.core import binder\n","binder.bind(globals())\n","from learntools.machine_learning.ex6 import *\n","print(\"\\nSetup complete\")"]},{"cell_type":"markdown","id":"3b1a2765","metadata":{"papermill":{"duration":0.003578,"end_time":"2023-07-05T04:08:02.931389","exception":false,"start_time":"2023-07-05T04:08:02.927811","status":"completed"},"tags":[]},"source":["Decision trees leave you with a difficult decision. A deep tree with lots of leaves will overfit because each prediction is coming from historical data from only the few houses at its leaf. But a shallow tree with few leaves will perform poorly because it fails to capture as many distinctions in the raw data.\n","\n","Even today's most sophisticated modeling techniques face this tension between underfitting and overfitting. But, many models have clever ideas that can lead to better performance. We'll look at the random forest as an example.\n","\n","The random forest uses many trees, and it makes a prediction by averaging the predictions of each component tree. It generally has much better predictive accuracy than a single decision tree and it works well with default parameters. If you keep modeling, you can learn more models with even better performance, but many of those are sensitive to getting the right parameters."]},{"cell_type":"markdown","id":"b6b03535","metadata":{"papermill":{"duration":0.003709,"end_time":"2023-07-05T04:08:02.938836","exception":false,"start_time":"2023-07-05T04:08:02.935127","status":"completed"},"tags":[]},"source":["# Exercises\n","Data science isn't always this easy. But replacing the decision tree with a Random Forest is going to be an easy win."]},{"cell_type":"markdown","id":"fbb15c3c","metadata":{"papermill":{"duration":0.003452,"end_time":"2023-07-05T04:08:02.945972","exception":false,"start_time":"2023-07-05T04:08:02.94252","status":"completed"},"tags":[]},"source":["We build a random forest model similarly to how we built a decision tree in scikit-learn - this time using the RandomForestRegressor class instead of DecisionTreeRegressor."]},{"cell_type":"markdown","id":"5fbbb209","metadata":{"papermill":{"duration":0.003356,"end_time":"2023-07-05T04:08:02.953008","exception":false,"start_time":"2023-07-05T04:08:02.949652","status":"completed"},"tags":[]},"source":["## Step 1: Use a Random Forest"]},{"cell_type":"code","execution_count":2,"id":"584775d2","metadata":{"execution":{"iopub.execute_input":"2023-07-05T04:08:02.961881Z","iopub.status.busy":"2023-07-05T04:08:02.961489Z","iopub.status.idle":"2023-07-05T04:08:03.630763Z","shell.execute_reply":"2023-07-05T04:08:03.629739Z"},"papermill":{"duration":0.676771,"end_time":"2023-07-05T04:08:03.633372","exception":false,"start_time":"2023-07-05T04:08:02.956601","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Validation MAE for Random Forest Model: 21857.15912981083\n"]},{"data":{"application/javascript":["parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 1.0, \"interactionType\": 1, \"questionType\": 2, \"questionId\": \"1_CheckRfScore\", \"learnToolsVersion\": \"0.3.4\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/markdown":["<span style=\"color:#33cc33\">Correct</span>"],"text/plain":["Correct"]},"metadata":{},"output_type":"display_data"}],"source":["from sklearn.ensemble import RandomForestRegressor\n","from sklearn.metrics import mean_absolute_error\n","\n","# Define the model. Set random_state to 1\n","rf_model = RandomForestRegressor(random_state=1)\n","\n","# fit your model\n","rf_model.fit(train_X,train_y)\n","\n","# Calculate the mean absolute error of your Random Forest model on the validation data\n","val_pred = rf_model.predict(val_X)\n","rf_val_mae = mean_absolute_error(val_pred, val_y)\n","\n","print(\"Validation MAE for Random Forest Model: {}\".format(rf_val_mae))\n","\n","# Check your answer\n","step_1.check()"]},{"cell_type":"markdown","id":"be807253","metadata":{"papermill":{"duration":0.003929,"end_time":"2023-07-05T04:08:03.641529","exception":false,"start_time":"2023-07-05T04:08:03.6376","status":"completed"},"tags":[]},"source":["In machine learning, a typical workflow involves three main steps: model creation, model training (or fitting), and model prediction. Let's break down each step and explain their differences:\n","\n","- Model Creation: This step involves selecting a machine learning algorithm or model that best suits the problem at hand. In your case, it seems like you have chosen a forest model, which could refer to a random forest or another type of ensemble tree-based model.\n","\n","- Model Training (or Fitting): Once you have created the model, you need to train it using labeled data. In this step, you provide the model with a training dataset (train_X and train_y in your code snippet) containing input features (train_X) and corresponding target labels or values (train_y). The model learns from this data by adjusting its internal parameters to minimize the difference between its predictions and the actual target values. The fit() method is typically used to train the model. It takes the training data as input and adjusts the model's parameters accordingly.\n","\n","- Model Prediction: After training the model, you can use it to make predictions on new, unseen data. In your code snippet, the predict() method is applied to the validation dataset (val_X). The model uses its learned parameters to generate predictions (melb_preds) for the target variable based on the provided input features (val_X).\n","\n","To summarize, model creation involves selecting an appropriate algorithm or model, model training (or fitting) involves adjusting the model's parameters based on labeled training data, and model prediction involves applying the trained model to new, unseen data to generate predictions."]},{"cell_type":"code","execution_count":3,"id":"72ca5aab","metadata":{"execution":{"iopub.execute_input":"2023-07-05T04:08:03.652827Z","iopub.status.busy":"2023-07-05T04:08:03.652462Z","iopub.status.idle":"2023-07-05T04:08:03.660661Z","shell.execute_reply":"2023-07-05T04:08:03.659678Z"},"papermill":{"duration":0.01608,"end_time":"2023-07-05T04:08:03.663273","exception":false,"start_time":"2023-07-05T04:08:03.647193","status":"completed"},"tags":[]},"outputs":[{"data":{"application/javascript":["parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"interactionType\": 2, \"questionType\": 2, \"questionId\": \"1_CheckRfScore\", \"learnToolsVersion\": \"0.3.4\", \"valueTowardsCompletion\": 0.0, \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\", \"outcomeType\": 4}}, \"*\")"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/markdown":["<span style=\"color:#3366cc\">Hint:</span> Review the code above with a DecisionTreeRegressor. Use the RandomForestRegressor instead"],"text/plain":["Hint: Review the code above with a DecisionTreeRegressor. Use the RandomForestRegressor instead"]},"metadata":{},"output_type":"display_data"}],"source":["# The lines below will show you a hint or the solution.\n","step_1.hint() \n","# step_1.solution()\n"]},{"cell_type":"markdown","id":"dde09f8b","metadata":{"papermill":{"duration":0.004315,"end_time":"2023-07-05T04:08:03.672126","exception":false,"start_time":"2023-07-05T04:08:03.667811","status":"completed"},"tags":[]},"source":["So far, you have followed specific instructions at each step of your project. This helped learn key ideas and build your first model, but now you know enough to try things on your own. \n","\n","Machine Learning competitions are a great way to try your own ideas and learn more as you independently navigate a machine learning project. \n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"},"papermill":{"default_parameters":{},"duration":16.687885,"end_time":"2023-07-05T04:08:06.388789","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-07-05T04:07:49.700904","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}